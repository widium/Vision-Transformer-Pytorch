{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer Model\n",
    "![](https://i.imgur.com/Nku8bAW.png)\n",
    "- Tokenize Input Image:\n",
    "Tokenize the input image using the `ImageTokenizer`, which extracts non-overlapping patches and converts them into a sequence of patch tokens (embeddings).\n",
    "\n",
    "- Prepare Token Sequence:\n",
    "Prepend the class token to the sequence of patch tokens using the `ClassTokenPrepender`.\n",
    "Add positional embeddings to the token sequence using the `PositionalEmbedding`.\n",
    "Apply dropout to the embeddings of tokens using the `Dropout` layer.\n",
    "\n",
    "- Encode Tokens:\n",
    "Encode the sequence of tokens (including the class token) using the `TransformerEncoder`, which consists of N `TransformerEncoderBlock` instances. The encoding process **captures global contextual information and relationships between the tokens.**\n",
    "\n",
    "- Classification:\n",
    "Extract the class token from the encoded sequence of tokens.\n",
    "Produce the output classification logits using the class token and the classifier (a linear layer followed by a normalization layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "from modeling.builder.image_tokenization import ImageTokenizer\n",
    "from modeling.builder.tokens_processing import ClassTokenPrepender\n",
    "from modeling.builder.tokens_processing import PositionalEmbedding\n",
    "from modeling.builder.encoder import TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VisionTransformerClassifier(Module): \n",
    "    \"\"\"\n",
    "    Vision Transformer model from Paper \"https://arxiv.org/abs/2010.11929\" for image Classification\n",
    "\n",
    "    Args:\n",
    "        `nbr_classes` (int): Number of classes for classification.\n",
    "        `height` (int, optional): Height of the input image. Defaults to 224.\n",
    "        `width` (int, optional): Width of the input image. Defaults to 224.\n",
    "        `color_channels` (int, optional): Number of color channels in the input image. Defaults to 3.\n",
    "        `patch_size` (int, optional): Size of the patches to divide the input image into. Defaults to 16.\n",
    "        `embedding_size` (int, optional): Size of the token embeddings. Defaults to 768.\n",
    "        `nbr_encoder_blocks` (int, optional): Number of transformer encoder blocks. Defaults to 12.\n",
    "        `nbr_heads` (int, optional): Number of attention heads in the transformer. Defaults to 12.\n",
    "        `mlp_units` (int, optional): Number of units in the mlp block inside the transformer. Defaults to 3072.\n",
    "        `dropout_embedding` (float, optional): Dropout rate for the embeddings. Defaults to 0.1.\n",
    "        `dropout_attention` (float, optional): Dropout rate for the attention block. Defaults to 0.0.\n",
    "        `dropout_mlp` (float, optional): Dropout rate for the mlp block. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        nbr_classes : int,\n",
    "        height : int = 224,\n",
    "        width : int = 224,\n",
    "        color_channels : int = 3,\n",
    "        patch_size : int = 16,\n",
    "        embedding_size : int = 768,\n",
    "        nbr_encoder_blocks : int = 12,\n",
    "        nbr_heads : int = 12,\n",
    "        mlp_units : int = 3072,\n",
    "        dropout_embedding : int = 0.1,\n",
    "        dropout_attention : float = 0.0,\n",
    "        dropout_mlp : float = 0.1,\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.nbr_patches = int((height * width) / (patch_size**2))\n",
    "        \n",
    "        self.image_tokenizer = ImageTokenizer(\n",
    "            patch_size=patch_size,\n",
    "            color_channels=color_channels,\n",
    "            embedding_size=embedding_size,\n",
    "        )\n",
    "        \n",
    "        self.class_token_prepender = ClassTokenPrepender(\n",
    "            embedding_size=embedding_size,\n",
    "        )\n",
    "        \n",
    "        self.positional_embedding = PositionalEmbedding(\n",
    "            nbr_token=self.nbr_patches + 1, # +1 for add class token\n",
    "            embedding_size=embedding_size,\n",
    "        )\n",
    "        \n",
    "        self.embedding_dropout = Dropout(p=dropout_embedding)\n",
    "        \n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            nbr_encoder_blocks=nbr_encoder_blocks,\n",
    "            embedding_size=embedding_size,\n",
    "            nbr_heads=nbr_heads,\n",
    "            dropout_attention=dropout_attention,\n",
    "            mlp_units=mlp_units,\n",
    "            dropout_mlp=dropout_mlp,\n",
    "        )\n",
    "        \n",
    "        self.classifier = Sequential(\n",
    "            LayerNorm(normalized_shape=embedding_size),\n",
    "            Linear(in_features=embedding_size, out_features=nbr_classes)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x : Tensor)->Tensor:\n",
    "        \"\"\"\n",
    "        1. Tokenize the input image using the `ImageTokenizer`, which extracts patches and converts them into tokens.\n",
    "        2. Prepend the class token to the sequence of patch tokens using the `ClassTokenPrepender`.\n",
    "        3. Add positional embeddings to the token sequence using the `PositionalEmbedding`.\n",
    "        4. Apply dropout to the embeddings of tokens using `Dropout` layer.\n",
    "        5. Encoding the sequence of tokens (including the class token) using the `TransformerEncoder`.\n",
    "        6. Extract the class token from the encoded sequence of tokens.\n",
    "        7. Produce the output classification logits using the class token and the classifier.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input image tensor with shape (batch_size, color_channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output classification logits with shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        tokens = self.image_tokenizer(x)\n",
    "        \n",
    "        tokens = self.class_token_prepender(tokens)\n",
    "        tokens = self.positional_embedding(tokens)\n",
    "        tokens = self.embedding_dropout(tokens)\n",
    "        \n",
    "        tokens_encoded = self.transformer_encoder(tokens)\n",
    "        class_token_encoded = tokens_encoded[:, 0] # Extract Class Token\n",
    "        \n",
    "        classification = self.classifier(class_token_encoded) # Use Class token for the Classification\n",
    "        \n",
    "        return (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n",
       "======================================================================================================================================================\n",
       "VisionTransformerClassifier (VisionTransformerClassifier)              [1, 3, 224, 224]     [1, 2]               --                   True\n",
       "├─ImageTokenizer (image_tokenizer)                                     [1, 3, 224, 224]     [1, 196, 768]        --                   True\n",
       "│    └─PatchExtractor (patch_extractor)                                [1, 3, 224, 224]     [1, 3, 14, 14]       --                   True\n",
       "│    │    └─Conv2d (patch_extractor)                                   [1, 3, 224, 224]     [1, 3, 14, 14]       2,307                True\n",
       "│    └─PatchTokenizer (patch_tokenizer)                                [1, 3, 14, 14]       [1, 196, 3]          --                   --\n",
       "│    │    └─Flatten (flatten)                                          [1, 3, 14, 14]       [1, 3, 196]          --                   --\n",
       "│    └─PatchTokenEmbedding (token_embedding)                           [1, 196, 3]          [1, 196, 768]        --                   True\n",
       "│    │    └─Linear (embedding)                                         [1, 196, 3]          [1, 196, 768]        3,072                True\n",
       "├─ClassTokenPrepender (class_token_prepender)                          [1, 196, 768]        [1, 197, 768]        768                  True\n",
       "├─PositionalEmbedding (positional_embedding)                           [1, 197, 768]        [1, 197, 768]        151,296              True\n",
       "├─Dropout (embedding_dropout)                                          [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "├─TransformerEncoder (transformer_encoder)                             [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─Sequential (encoder_blocks)                                     [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─TransformerEncoderBlock (0)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (1)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (2)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (3)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (4)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (5)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (6)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (7)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (8)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (9)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (10)                               [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (11)                               [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "├─Sequential (classifier)                                              [1, 768]             [1, 2]               --                   True\n",
       "│    └─LayerNorm (0)                                                   [1, 768]             [1, 768]             1,536                True\n",
       "│    └─Linear (1)                                                      [1, 768]             [1, 2]               1,538                True\n",
       "======================================================================================================================================================\n",
       "Total params: 85,214,981\n",
       "Trainable params: 85,214,981\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 57.16\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 105.31\n",
       "Params size (MB): 227.47\n",
       "Estimated Total Size (MB): 333.38\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "ViT = VisionTransformerClassifier(\n",
    "        nbr_classes=2,\n",
    "        height=224, \n",
    "        width=224,\n",
    "        color_channels=3,\n",
    "        patch_size=16,\n",
    "        embedding_size=768,\n",
    "        nbr_encoder_blocks=12,\n",
    "        nbr_heads=12,\n",
    "        mlp_units=3072,\n",
    "        dropout_embedding=0.1,\n",
    "        dropout_attention=0.0,\n",
    "        dropout_mlp=0.1\n",
    ")\n",
    "\n",
    "summary(model=ViT, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/T77WAEX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3106, -0.4393]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = torch.randn(size=(1, 3, 224, 224)).to(device=\"cuda\")\n",
    "\n",
    "prediction = ViT(dummy)\n",
    "\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
