{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection\n",
    "![](https://i.imgur.com/m7KQiX8.png)\n",
    "Residual connections enabling gradients to flow more efficiently, fix the vanishing gradient problem and promoting learning of incremental features at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "class ResidualConnection(Module):\n",
    "    \"\"\"\n",
    "    Apply residual connection with element-wise addition between input tensor and output tensor\n",
    "    \n",
    "    Return :\n",
    "        Tensor : same shape of input and output tensor\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input : Tensor, output : Tensor)->Tensor:\n",
    "        \n",
    "        residual_connection = torch.add(input, output)\n",
    "        \n",
    "        return (residual_connection)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 768\n",
    "\n",
    "x = torch.randn(size=(1, 5, embedding_size))\n",
    "output = torch.randn(size=(1, 5, embedding_size))\n",
    "\n",
    "layer = ResidualConnection()\n",
    "\n",
    "residual_output = layer(input=x, output=output)\n",
    "\n",
    "residual_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Self Attention Block\n",
    "![](https://i.imgur.com/xGGHbdX.png)\n",
    "#### *Normalize input feature and apply multi-head self-attention followed by a residual connection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.nn import LayerNorm\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "class MultiHeadSelfAttentionBlock(Module):\n",
    "    \"\"\"\n",
    "    Normalize input feature and apply multi-head self-attention followed by a residual connection.\n",
    "    The input and output tensor shapes are expected to be identical.\n",
    "\n",
    "    Attributes:\n",
    "        `layer_norm` (LayerNorm): Layer normalization applied to the input tensor.\n",
    "        `multi_head_attention` (MultiheadAttention): Multi-head self-attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        `embedding_size` (int, optional): the input tensor's last dimension, which corresponds\n",
    "            to the number of features or embedding dimensions. Default is 768.\n",
    "        `num_heads` (int, optional): The number of attention heads in the multi-head self-attention mechanism.\n",
    "            Default is 12.\n",
    "        `dropout_rate` (float, optional): The dropout rate on attention.\n",
    "            Default is 0.1.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The output tensor after applying multi-head self-attention and the residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_size : int = 768, \n",
    "        nbr_heads : int = 12, \n",
    "        dropout_rate : float = 0.1,\n",
    "    )->None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = LayerNorm(normalized_shape=embedding_size)\n",
    "        \n",
    "        self.multi_head_attention = MultiheadAttention(\n",
    "            embed_dim=embedding_size,\n",
    "            num_heads=nbr_heads,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.residual_connection = ResidualConnection()\n",
    "        \n",
    "    \n",
    "    def forward(self, x : Tensor)->Tensor:\n",
    "        \n",
    "        x_normalized = self.layer_norm(x)\n",
    "        \n",
    "        attention, _ = self.multi_head_attention(# duplicate the input tensor `x_normalized` for Self Attention\n",
    "            query=x_normalized,\n",
    "            key=x_normalized,\n",
    "            value=x_normalized,\n",
    "            need_weights=False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        residual_output = self.residual_connection(input=x, output=attention)\n",
    "        \n",
    "        return (residual_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 768\n",
    "\n",
    "x = torch.randn(size=(1, 5, embedding_size))\n",
    "\n",
    "attention_block = MultiHeadSelfAttentionBlock(\n",
    "    embedding_size=embedding_size,\n",
    "    nbr_heads=12,\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "\n",
    "out = attention_block(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #\n",
       "========================================================================================================================\n",
       "MultiHeadSelfAttentionBlock (MultiHeadSelfAttentionBlock)    [1, 5, 768]          [1, 5, 768]          --\n",
       "├─LayerNorm (layer_norm)                                     [1, 5, 768]          [1, 5, 768]          1,536\n",
       "├─MultiheadAttention (multi_head_attention)                  --                   [1, 5, 768]          2,362,368\n",
       "├─ResidualConnection (residual_connection)                   --                   [1, 5, 768]          --\n",
       "========================================================================================================================\n",
       "Total params: 2,363,904\n",
       "Trainable params: 2,363,904\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.05\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=attention_block, \n",
    "        input_size=(1, 5, embedding_size),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why `GELU` *in a nutshell* ?\n",
    "![](https://i.imgur.com/n9C9j1M.png)\n",
    "*$$\\text{Gradient Everywhere}$$*\n",
    "$$GELU=0.5 x\\left(1+\\frac{2}{\\sqrt{\\pi}} \\int_0^{\\frac{x}{\\sqrt{2}}} e^{-t^2} d t\\right)$$\n",
    "- The GELU activation function is preferred over ReLU to its smooth differentiability, better gradient flow, and give ability to model to learn complex patterns with long-term dependencies in sequence data. \n",
    "- GELU is commonly used in Transformer architectures like BERT and tasks requiring complex nonlinear modeling. \n",
    "***\n",
    "### Multi Layer Perceptron Block (MLP)\n",
    "![](https://i.imgur.com/RNp7TIb.png)\n",
    "\n",
    "The MLP block comprises two fully connected layers with a GELU activation \n",
    "- Layer normalization is applied to the input features before entering the MLP block\n",
    "- The first fully connected layer processes the input tensor, generating an intermediate tensor that is then passed through a GELU activation function.\n",
    "- The second fully connected layer processes intermediate tensor, generating an output tensor.\n",
    "- Dropout is applied after each feature creation (like Linear Layer), helping to prevent overfitting.\n",
    "- Residual connections are used on output tensor of the second Layer with input tensor\n",
    "****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "from torch.nn import GELU\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "class MultiLayerPerceptronBlock(Module):  \n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron (MLP) block with residual connections, used in the Transformer Encoder.\n",
    "    \n",
    "    The MLP block consists of two fully connected layers with a GELU activation function in between.\n",
    "    Layer normalization is applied on input features before the block, \n",
    "    Dropout is applied after each feature creation (like Linear Layer)\n",
    "    Residual connections are used on output of the second Layer with input tensor\n",
    "    \n",
    "    Args:\n",
    "        `embedding_size` (int, optional): Input and output tensor size. Default is 768.\n",
    "        `units` (int, optional): Number of hidden units in the intermediate layer. Default is 3072.\n",
    "        `dropout_rate` (float, optional): Dropout probability. Default is 0.1.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: Output tensor with the same shape as the input tensor (batch_size, nbr_tokens, embedding_size).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_size: int = 768, \n",
    "        units: int = 3072, \n",
    "        dropout_rate: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = LayerNorm(normalized_shape=embedding_size)\n",
    "        self.gelu = GELU()\n",
    "        self.dropout = Dropout(p=dropout_rate)\n",
    "        \n",
    "        self.fc1 = Linear(\n",
    "            in_features=embedding_size,\n",
    "            out_features=units\n",
    "        )\n",
    "        \n",
    "        self.fc2 = Linear(\n",
    "            in_features=units,\n",
    "            out_features=embedding_size\n",
    "        )\n",
    "        \n",
    "        self.residual_connection = ResidualConnection()\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        x_norm = self.layer_norm(x)\n",
    "        \n",
    "        hidden = self.fc1(x_norm)\n",
    "        hidden = self.gelu(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        output = self.fc2(hidden)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        residual_output = self.residual_connection(input=x, output=output)\n",
    "        \n",
    "        return (residual_output)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 768\n",
    "\n",
    "x = torch.randn(size=(1, 5, embedding_size))\n",
    "\n",
    "mlp_block = MultiLayerPerceptronBlock(\n",
    "    embedding_size=embedding_size,\n",
    "    units=3072,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "out = mlp_block(x)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #\n",
       "===================================================================================================================\n",
       "MultiLayerPerceptronBlock (MultiLayerPerceptronBlock)   [1, 5, 768]          [1, 5, 768]          --\n",
       "├─LayerNorm (layer_norm)                                [1, 5, 768]          [1, 5, 768]          1,536\n",
       "├─Linear (fc1)                                          [1, 5, 768]          [1, 5, 3072]         2,362,368\n",
       "├─GELU (gelu)                                           [1, 5, 3072]         [1, 5, 3072]         --\n",
       "├─Dropout (dropout)                                     [1, 5, 3072]         [1, 5, 3072]         --\n",
       "├─Linear (fc2)                                          [1, 5, 3072]         [1, 5, 768]          2,360,064\n",
       "├─Dropout (dropout)                                     [1, 5, 768]          [1, 5, 768]          --\n",
       "├─ResidualConnection (residual_connection)              --                   [1, 5, 768]          --\n",
       "===================================================================================================================\n",
       "Total params: 4,723,968\n",
       "Trainable params: 4,723,968\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.72\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.18\n",
       "Params size (MB): 18.90\n",
       "Estimated Total Size (MB): 19.10\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=mlp_block, \n",
    "        input_size=(1, 5, embedding_size),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Block \n",
    "![](https://i.imgur.com/UNfcDsq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "class TransformerEncoderBlock(Module):   \n",
    "    \"\"\"\n",
    "    building block that combines multi-head self-attention and a multi-layer perceptron, \n",
    "    applying residual connections and layer normalization. \n",
    "\n",
    "    Args:\n",
    "        `embedding_size` (int, optional): The size of the input embeddings. Default is 768.\n",
    "        `nbr_heads` (int, optional): The number of attention heads in the `MultiHeadSelfAttentionBlock`. Default is 12.\n",
    "        `dropout_attention` (float, optional): The dropout rate for the `MultiHeadSelfAttentionBlock`. Default is 0.0.\n",
    "        `mlp_units` (int, optional): The number of units in the `MultiLayerPerceptronBlock`. Default is 3072.\n",
    "        `dropout_mlp` (float, optional): The dropout rate for the `MultiLayerPerceptronBlock`. Default is 0.1.\n",
    "    \n",
    "    Forward method input:\n",
    "        x (Tensor): a sequence of Tokens Embedding like [batch_size, nbr_tokens, embedding_size].\n",
    "    \n",
    "    Return\n",
    "        Tensor: a tensor with same input size after passing through the `MultiHeadSelfAttentionBlock` and` MultiLayerPerceptronBlock`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_size : int = 768, \n",
    "        nbr_heads : int = 12,\n",
    "        dropout_attention : float = 0.0,\n",
    "        mlp_units : int = 3072,\n",
    "        dropout_mlp : float = 0.1,\n",
    "    )->None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_block = MultiHeadSelfAttentionBlock(\n",
    "            embedding_size=embedding_size,\n",
    "            nbr_heads=nbr_heads,\n",
    "            dropout_rate=dropout_attention\n",
    "        )\n",
    "        \n",
    "        self.mlp_block = MultiLayerPerceptronBlock(\n",
    "            embedding_size=embedding_size,\n",
    "            units=mlp_units,\n",
    "            dropout_rate=dropout_mlp,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x : Tensor)->Tensor:\n",
    "        \n",
    "        attention = self.attention_block(x)\n",
    "        new_features = self.mlp_block(attention)\n",
    "    \n",
    "        return (new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 768\n",
    "\n",
    "x = torch.randn(size=(1, 5, embedding_size))\n",
    "\n",
    "encoder_block = TransformerEncoderBlock(\n",
    "    embedding_size=embedding_size,\n",
    "    nbr_heads=12,\n",
    "    dropout_attention=0.0,\n",
    "    mlp_units=3072,\n",
    "    dropout_mlp=0.1\n",
    ")\n",
    "\n",
    "out = encoder_block(x)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #\n",
       "===================================================================================================================\n",
       "TransformerEncoderBlock (TransformerEncoderBlock)       [1, 5, 768]          [1, 5, 768]          --\n",
       "├─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          --\n",
       "│    └─LayerNorm (layer_norm)                           [1, 5, 768]          [1, 5, 768]          1,536\n",
       "│    └─MultiheadAttention (multi_head_attention)        --                   [1, 5, 768]          2,362,368\n",
       "│    └─ResidualConnection (residual_connection)         --                   [1, 5, 768]          --\n",
       "├─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          --\n",
       "│    └─LayerNorm (layer_norm)                           [1, 5, 768]          [1, 5, 768]          1,536\n",
       "│    └─Linear (fc1)                                     [1, 5, 768]          [1, 5, 3072]         2,362,368\n",
       "│    └─GELU (gelu)                                      [1, 5, 3072]         [1, 5, 3072]         --\n",
       "│    └─Dropout (dropout)                                [1, 5, 3072]         [1, 5, 3072]         --\n",
       "│    └─Linear (fc2)                                     [1, 5, 3072]         [1, 5, 768]          2,360,064\n",
       "│    └─Dropout (dropout)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    └─ResidualConnection (residual_connection)         --                   [1, 5, 768]          --\n",
       "===================================================================================================================\n",
       "Total params: 7,087,872\n",
       "Trainable params: 7,087,872\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.73\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.22\n",
       "Params size (MB): 18.90\n",
       "Estimated Total Size (MB): 19.13\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=encoder_block, \n",
    "        input_size=(1, 5, embedding_size),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "![](https://i.imgur.com/HXvoxHr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.nn import Sequential\n",
    "\n",
    "class TransformerEncoder(Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        nbr_encoder_blocks : int = 12,\n",
    "        embedding_size : int = 768, \n",
    "        nbr_heads : int = 12,\n",
    "        dropout_attention : float = 0.0,\n",
    "        mlp_units : int = 3072,\n",
    "        dropout_mlp : float = 0.1,\n",
    "    )->None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Sequential()\n",
    "        \n",
    "        for _ in range(nbr_encoder_blocks):\n",
    "            \n",
    "            block = TransformerEncoderBlock(\n",
    "                embedding_size=embedding_size,\n",
    "                nbr_heads=nbr_heads,\n",
    "                dropout_attention=dropout_attention,\n",
    "                mlp_units=mlp_units,\n",
    "                dropout_mlp=dropout_mlp\n",
    "            )\n",
    "            \n",
    "            self.encoder.append(module=block)\n",
    "        \n",
    "    def forward(self, x : Tensor)->Tensor:\n",
    "        \n",
    "        output = self.encoder(x)\n",
    "        \n",
    "        return (output)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 768\n",
    "\n",
    "x = torch.randn(size=(1, 5, embedding_size))\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    embedding_size=embedding_size,\n",
    "    nbr_heads=12,\n",
    "    dropout_attention=0.0,\n",
    "    mlp_units=3072,\n",
    "    dropout_mlp=0.1\n",
    ")\n",
    "\n",
    "out = encoder(x)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape          Output Shape         Param #\n",
       "=============================================================================================================================\n",
       "TransformerEncoder (TransformerEncoder)                           [1, 5, 768]          [1, 5, 768]          --\n",
       "├─Sequential (encoder)                                            [1, 5, 768]          [1, 5, 768]          --\n",
       "│    └─TransformerEncoderBlock (0)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (1)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (2)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (3)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (4)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (5)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (6)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (7)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (8)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (9)                                [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (10)                               [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "│    └─TransformerEncoderBlock (11)                               [1, 5, 768]          [1, 5, 768]          --\n",
       "│    │    └─MultiHeadSelfAttentionBlock (attention_block)         [1, 5, 768]          [1, 5, 768]          2,363,904\n",
       "│    │    └─MultiLayerPerceptronBlock (mlp_block)                 [1, 5, 768]          [1, 5, 768]          4,723,968\n",
       "=============================================================================================================================\n",
       "Total params: 85,054,464\n",
       "Trainable params: 85,054,464\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 56.71\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 2.58\n",
       "Params size (MB): 226.82\n",
       "Estimated Total Size (MB): 229.42\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=encoder_block, \n",
    "        input_size=(1, 5, embedding_size),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
