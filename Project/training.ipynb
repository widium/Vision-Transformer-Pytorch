{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Path of source code file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : add [/home/widium/Programming/AI/Vision-Transformer/Project] to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def add_top_directory_to_python_path(top_directory : str = \"Project\"):\n",
    "    \"\"\"\n",
    "    Add top directory of Project to Python Path for managing import between directory\n",
    "\n",
    "    Args:\n",
    "        top_directory (str, optional): top folder of project. Defaults to \"Project\".\n",
    "    \"\"\"\n",
    "    current_dir_path = Path(os.getcwd())\n",
    "    current_dir_name = current_dir_path.stem\n",
    "    \n",
    "    while (current_dir_name != top_directory):\n",
    "        current_dir_path = current_dir_path.parent\n",
    "        current_dir_name = current_dir_path.stem\n",
    "    \n",
    "    sys.path.append(str(current_dir_path))\n",
    "    print(f\"[INFO] : add [{current_dir_path}] to Python path\")\n",
    "\n",
    "add_top_directory_to_python_path(top_directory=\"Project\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SAVING_PATH = Path(\"data/dataset/\")\n",
    "\n",
    "train_dataset_path = SAVING_PATH / \"train_dataset.pth\"\n",
    "test_dataset_path = SAVING_PATH / \"test_dataset.pth\"\n",
    "\n",
    "train_dataloader = torch.load(f=train_dataset_path)\n",
    "test_dataloader = torch.load(f=test_dataset_path)\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ViT Base Instance\n",
    "![](https://i.imgur.com/GLaAgax.png)\n",
    "\n",
    "- Define the Hyperparameters Constante for recreate the ViTBase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_CLASS = 3\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "COLOR = 3\n",
    "PATCH_SIZE = 16\n",
    "EMBEDDING = 768\n",
    "NBR_ENCODER_BLOCK = 12\n",
    "NBR_HEADS = 12\n",
    "MLP_UNITS = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.builder.vit_model import VisionTransformerClassifier\n",
    "\n",
    "vit_base = VisionTransformerClassifier(\n",
    "    nbr_classes=NBR_CLASS,\n",
    "    height=HEIGHT,\n",
    "    width=WIDTH,\n",
    "    color_channels=COLOR,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    embedding_size=EMBEDDING,\n",
    "    nbr_encoder_blocks=NBR_ENCODER_BLOCK,\n",
    "    nbr_heads=NBR_HEADS ,\n",
    "    mlp_units=MLP_UNITS,\n",
    "    dropout_embedding=0.1,\n",
    "    dropout_attention=0.0,\n",
    "    dropout_mlp=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n",
       "======================================================================================================================================================\n",
       "VisionTransformerClassifier (VisionTransformerClassifier)              [1, 3, 224, 224]     [1, 3]               --                   True\n",
       "├─ImageTokenizer (image_tokenizer)                                     [1, 3, 224, 224]     [1, 196, 768]        --                   True\n",
       "│    └─PatchExtractor (patch_extractor)                                [1, 3, 224, 224]     [1, 3, 14, 14]       --                   True\n",
       "│    │    └─Conv2d (patch_extractor)                                   [1, 3, 224, 224]     [1, 3, 14, 14]       2,307                True\n",
       "│    └─PatchTokenizer (patch_tokenizer)                                [1, 3, 14, 14]       [1, 196, 3]          --                   --\n",
       "│    │    └─Flatten (flatten)                                          [1, 3, 14, 14]       [1, 3, 196]          --                   --\n",
       "│    └─PatchTokenEmbedding (token_embedding)                           [1, 196, 3]          [1, 196, 768]        --                   True\n",
       "│    │    └─Linear (embedding)                                         [1, 196, 3]          [1, 196, 768]        3,072                True\n",
       "├─ClassTokenPrepender (class_token_prepender)                          [1, 196, 768]        [1, 197, 768]        768                  True\n",
       "├─PositionalEmbedding (positional_embedding)                           [1, 197, 768]        [1, 197, 768]        151,296              True\n",
       "├─Dropout (embedding_dropout)                                          [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "├─TransformerEncoder (transformer_encoder)                             [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─Sequential (encoder_blocks)                                     [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─TransformerEncoderBlock (0)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (1)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (2)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (3)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (4)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (5)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (6)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (7)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (8)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (9)                                [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (10)                               [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─TransformerEncoderBlock (11)                               [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "├─Sequential (classifier)                                              [1, 768]             [1, 3]               --                   True\n",
       "│    └─LayerNorm (0)                                                   [1, 768]             [1, 768]             1,536                True\n",
       "│    └─Linear (1)                                                      [1, 768]             [1, 3]               2,307                True\n",
       "======================================================================================================================================================\n",
       "Total params: 85,215,750\n",
       "Trainable params: 85,215,750\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 57.17\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 105.31\n",
       "Params size (MB): 227.47\n",
       "Estimated Total Size (MB): 333.38\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=vit_base, \n",
    "        input_size=(1, COLOR, HEIGHT, WIDTH),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Loss Function, Optimizer and Metrics for Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics import Accuracy\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "optimizer = Adam(vit_base.parameters(), lr=0.001)\n",
    "loss_function = CrossEntropyLoss()\n",
    "metric_function = Accuracy(task=\"multiclass\", num_classes=NBR_CLASS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the `train` Function \n",
    "- setup device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "### Setup device agnostic code\n",
    "device = \"cpu\"\n",
    "\n",
    "vit_base.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 ** 2)  # Convert bytes to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 4905.39 MB\n"
     ]
    }
   ],
   "source": [
    "memory_usage = get_memory_usage()\n",
    "print(f\"Memory usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory capacity: 31.08 GB\n"
     ]
    }
   ],
   "source": [
    "def get_total_memory():\n",
    "    mem_info = psutil.virtual_memory()\n",
    "    return mem_info.total / (1024 ** 3)  # Convert bytes to GB\n",
    "\n",
    "total_memory = get_total_memory()\n",
    "print(f\"Total memory capacity: {total_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "def compute_size_of_model(model : Module)->dict:\n",
    "    \"\"\"compute the detailed size of Pytorch Model\n",
    "\n",
    "    Args:\n",
    "        model (Module): model\n",
    "\n",
    "    Returns:\n",
    "        dict: python dictionary with 3 size \n",
    "        - `params` : accumulate size of all trainable parameters in module\n",
    "        - `buffer` : accumulate size of all non-trainable tensors in module\n",
    "        - `entire` : params + buffer\n",
    "    \"\"\"\n",
    "    size = dict()\n",
    "    size[\"params\"] = 0\n",
    "    size[\"buffer\"] = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        size[\"params\"] += param.nelement() * param.element_size()\n",
    "\n",
    "    for buffer in model.buffers():\n",
    "        size[\"buffer\"] += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    # Convert to Bytes to MegaBytes\n",
    "    size[\"params\"] /= 1024**2\n",
    "    size[\"buffer\"] /= 1024**2\n",
    "    \n",
    "    # compute the entire size in MegaBytes\n",
    "    size[\"entire\"] =  size[\"params\"] + size[\"buffer\"]\n",
    "    \n",
    "    return (size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters size: 325.072 (MB)\n",
      "Model Utils size: 0.000 (MB)\n",
      "Model Entire Size: 325.072 (MB)\n"
     ]
    }
   ],
   "source": [
    "size = compute_size_of_model(vit_base)\n",
    "\n",
    "print(f'Model Parameters size: {size[\"params\"]:.3f} (MB)')\n",
    "print(f'Model Utils size: {size[\"buffer\"]:.3f} (MB)')\n",
    "print(f'Model Entire Size: {size[\"entire\"]:.3f} (MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbcb9fefb7e4fafbb6d409017975771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs            | Train Loss        | Train Accuracy    | Val Loss          | Val Accuracy      |\n",
      "===============================================================================================\n",
      "20.0% [1/5]       | 3.3105            | 0.3242            | 1.0139            | 0.5417            |\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from modeling.training.train import train\n",
    "\n",
    "history = train(\n",
    "    model=vit_base,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=metric_function,\n",
    "    device=device,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
